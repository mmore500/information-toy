\section{Conclusion}

There you have it.
Information is the difference between entropies.
Entropy is a quantification of uncertainty.
If a discrete random variable models a situation, calculate entropy can be calculated as a function of the set of probabilities associated with the possible outcomes of the situation.
Just plug that set of probabilities into Shannon's equation to calculate entropy.

I hope these illustrated examples have helped you get a hands-on feel for information and entropy.
Now, go check out \cite{Adami2012} for some nicely described applications of information theory to biology and evolution.
If you want to further firm up your footing with information theory itself, give \cite{Adami2016} a read.
Don't be afraid to make your own toy problems and analyze them yourself!
It's the best way to get comfortable with any new math topic.

What's the entropy associated with a fair six-sided die?
What's the information content of a three letter code if all three letters are drawn independently from a uniform distribution over all 26 letters?
What if the three letters are drawn without replacement?
If you're looking for a challenge, maybe check out the \href{https://en.wikipedia.org/wiki/Monty_Hall_problem}{Monty Hall problem}.
What's the entropy when all three doors are closed?
What's the entropy after the host opens a goat door?
How much information is gained when the host opens a goat door?
This one's tricky.
Have fun!
